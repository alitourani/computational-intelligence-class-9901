{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting blood's geloucos level using LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Members\n",
    "* Amir Mohammad Fallah (970122680014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "import pandas as pd \n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting time series to supervised problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=2, n_out=2, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('dataset/data.csv')\n",
    "# drop missing data\n",
    "dataframe = dataframe.dropna()\n",
    "dataframe = dataframe.drop(\"PtID\",axis=1)\n",
    "values = dataframe.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling geloucos data range of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaledContinues = scaler.fit_transform(dataframe[[\"Glucose\"]])\n",
    "reframed = series_to_supervised(scaledContinues,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "n_train_len = int(len(values) * 0.7)\n",
    "n_val_len = int(n_train_len * 0.1)\n",
    "\n",
    "\n",
    "train = values[:n_train_len - n_val_len, :]\n",
    "val = values[n_train_len - n_val_len:n_train_len, :]\n",
    "test = values[n_train_len:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44475138, 0.43093923, 0.5441989 ],\n",
       "       [0.43093923, 0.5441989 , 0.47237569],\n",
       "       [0.5441989 , 0.47237569, 0.43370166],\n",
       "       ...,\n",
       "       [0.72651934, 0.75966851, 0.77071823],\n",
       "       [0.75966851, 0.77071823, 0.79558011],\n",
       "       [0.77071823, 0.79558011, 0.79834254]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape input to be 3D [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.00001)\n",
    "\n",
    "\n",
    "model = None\n",
    "# building the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(65, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1 ,activation=\"linear\"))\n",
    "model.compile(loss = 'mean_absolute_error', optimizer = optimizer, metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit network on prepared sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25198 samples, validate on 2799 samples\n",
      "Epoch 1/100\n",
      "25198/25198 [==============================] - 2s 67us/step - loss: 0.3401 - mse: 0.1627 - val_loss: 0.3059 - val_mse: 0.1414\n",
      "Epoch 2/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.2146 - mse: 0.0757 - val_loss: 0.1669 - val_mse: 0.0501\n",
      "Epoch 3/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.1373 - mse: 0.0300 - val_loss: 0.1297 - val_mse: 0.0271\n",
      "Epoch 4/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.1169 - mse: 0.0205 - val_loss: 0.1121 - val_mse: 0.0193\n",
      "Epoch 5/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0996 - mse: 0.0149 - val_loss: 0.0920 - val_mse: 0.0130\n",
      "Epoch 6/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0776 - mse: 0.0094 - val_loss: 0.0657 - val_mse: 0.0070\n",
      "Epoch 7/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0516 - mse: 0.0048 - val_loss: 0.0366 - val_mse: 0.0030\n",
      "Epoch 8/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0306 - mse: 0.0027 - val_loss: 0.0237 - val_mse: 0.0022\n",
      "Epoch 9/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0265 - mse: 0.0025 - val_loss: 0.0226 - val_mse: 0.0021\n",
      "Epoch 10/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0255 - mse: 0.0024 - val_loss: 0.0218 - val_mse: 0.0020\n",
      "Epoch 11/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0246 - mse: 0.0023 - val_loss: 0.0210 - val_mse: 0.0020\n",
      "Epoch 12/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0238 - mse: 0.0022 - val_loss: 0.0203 - val_mse: 0.0019\n",
      "Epoch 13/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0231 - mse: 0.0021 - val_loss: 0.0197 - val_mse: 0.0018\n",
      "Epoch 14/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0225 - mse: 0.0021 - val_loss: 0.0192 - val_mse: 0.0018\n",
      "Epoch 15/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0219 - mse: 0.0020 - val_loss: 0.0186 - val_mse: 0.0018\n",
      "Epoch 16/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0213 - mse: 0.0020 - val_loss: 0.0182 - val_mse: 0.0017\n",
      "Epoch 17/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0209 - mse: 0.0019 - val_loss: 0.0178 - val_mse: 0.0017\n",
      "Epoch 18/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0204 - mse: 0.0019 - val_loss: 0.0173 - val_mse: 0.0016\n",
      "Epoch 19/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0199 - mse: 0.0018 - val_loss: 0.0170 - val_mse: 0.0016\n",
      "Epoch 20/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0195 - mse: 0.0018 - val_loss: 0.0165 - val_mse: 0.0016\n",
      "Epoch 21/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0190 - mse: 0.0018 - val_loss: 0.0162 - val_mse: 0.0015\n",
      "Epoch 22/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0185 - mse: 0.0017 - val_loss: 0.0158 - val_mse: 0.0015\n",
      "Epoch 23/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0182 - mse: 0.0017 - val_loss: 0.0154 - val_mse: 0.0015\n",
      "Epoch 24/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0179 - mse: 0.0017 - val_loss: 0.0152 - val_mse: 0.0015\n",
      "Epoch 25/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0176 - mse: 0.0017 - val_loss: 0.0150 - val_mse: 0.0015\n",
      "Epoch 26/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0174 - mse: 0.0017 - val_loss: 0.0148 - val_mse: 0.0015\n",
      "Epoch 27/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0173 - mse: 0.0017 - val_loss: 0.0146 - val_mse: 0.0015\n",
      "Epoch 28/100\n",
      "25198/25198 [==============================] - 1s 44us/step - loss: 0.0172 - mse: 0.0017 - val_loss: 0.0145 - val_mse: 0.0014\n",
      "Epoch 29/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0171 - mse: 0.0017 - val_loss: 0.0145 - val_mse: 0.0014\n",
      "Epoch 30/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0170 - mse: 0.0016 - val_loss: 0.0144 - val_mse: 0.0014\n",
      "Epoch 31/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0169 - mse: 0.0016 - val_loss: 0.0144 - val_mse: 0.0014\n",
      "Epoch 32/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0169 - mse: 0.0016 - val_loss: 0.0143 - val_mse: 0.0014\n",
      "Epoch 33/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0168 - mse: 0.0016 - val_loss: 0.0143 - val_mse: 0.0014\n",
      "Epoch 34/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0168 - mse: 0.0016 - val_loss: 0.0142 - val_mse: 0.0014\n",
      "Epoch 35/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0168 - mse: 0.0016 - val_loss: 0.0142 - val_mse: 0.0014\n",
      "Epoch 36/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0167 - mse: 0.0016 - val_loss: 0.0142 - val_mse: 0.0014\n",
      "Epoch 37/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0167 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 38/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0167 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 39/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0166 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 40/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0166 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 41/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0166 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 42/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0166 - mse: 0.0016 - val_loss: 0.0141 - val_mse: 0.0014\n",
      "Epoch 43/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0166 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 44/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 45/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 46/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 47/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 48/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0140 - val_mse: 0.0014\n",
      "Epoch 49/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0165 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 50/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 51/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 52/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 53/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 54/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 55/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 56/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0164 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 57/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 58/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 60/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 61/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 62/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 63/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 64/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 65/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0163 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 66/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 67/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 68/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 69/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 70/100\n",
      "25198/25198 [==============================] - 1s 49us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 71/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 72/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 73/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 74/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0162 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 75/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 76/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 77/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 78/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 79/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 80/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 81/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 82/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 83/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0136 - val_mse: 0.0014\n",
      "Epoch 84/100\n",
      "25198/25198 [==============================] - 1s 44us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0136 - val_mse: 0.0014\n",
      "Epoch 85/100\n",
      "25198/25198 [==============================] - 1s 44us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 86/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0136 - val_mse: 0.0014\n",
      "Epoch 87/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 88/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0136 - val_mse: 0.0014\n",
      "Epoch 89/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 90/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0136 - val_mse: 0.0014\n",
      "Epoch 91/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 92/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 93/100\n",
      "25198/25198 [==============================] - 1s 45us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 94/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 95/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 96/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0139 - val_mse: 0.0014\n",
      "Epoch 97/100\n",
      "25198/25198 [==============================] - 1s 46us/step - loss: 0.0161 - mse: 0.0016 - val_loss: 0.0138 - val_mse: 0.0014\n",
      "Epoch 98/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0160 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 99/100\n",
      "25198/25198 [==============================] - 1s 48us/step - loss: 0.0160 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n",
      "Epoch 100/100\n",
      "25198/25198 [==============================] - 1s 47us/step - loss: 0.0160 - mse: 0.0016 - val_loss: 0.0137 - val_mse: 0.0014\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, epochs=100, validation_data=(val_X, val_y), verbose=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.title(\"\")\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions and evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
